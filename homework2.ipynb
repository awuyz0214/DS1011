{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import csv\n",
    "import pandas as pd\n",
    "import io\n",
    "from numpy import random\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.backends.cudnn.enabled=False\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check GPU availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available and torch.has_cudnn:\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tsv(filename):\n",
    "    train_premise = []\n",
    "    train_hypo = []\n",
    "    train_label = []\n",
    "    with open(filename) as tsvfile:\n",
    "      fd = csv.reader(tsvfile, delimiter='\\t')\n",
    "      for row in fd:\n",
    "            \n",
    "        train_premise.append(row[0].split())\n",
    "        train_hypo.append(row[1].split())\n",
    "        \n",
    "        if row[2] == 'neutral': \n",
    "            train_label.append(1)\n",
    "        elif row[2] == 'entailment':\n",
    "            train_label.append(2)\n",
    "        else:\n",
    "            train_label.append(0)\n",
    "            \n",
    "    train_premise = train_premise[1:]\n",
    "    train_hypo = train_hypo[1:]\n",
    "    train_label = train_label[1:]\n",
    "    \n",
    "    return train_premise, train_hypo, train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_premise,train_hypo,train_label = load_tsv('snli_train.tsv')\n",
    "val_premise,val_hypo,val_label = load_tsv('snli_val.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = 40000\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "with open('wiki-news-300d-1M.vec') as f:\n",
    "    loaded_embeddings_ft = np.zeros((words + 2, 300))\n",
    "    words_ft = {}\n",
    "    idx2words_ft = {}\n",
    "    ordered_words_ft = []\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        if i >= words: \n",
    "            break\n",
    "        s = line.split()\n",
    "        loaded_embeddings_ft[i, :] = np.asarray(s[1:])\n",
    "        words_ft[s[0]] = i\n",
    "        idx2words_ft[i] = s[0]\n",
    "        ordered_words_ft.append(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_pad = len(words_ft)\n",
    "idx_unk = len(words_ft) + 1\n",
    "words_ft['<pad>'] = idx_pad\n",
    "words_ft['<unk>'] = idx_unk\n",
    "idx2words_ft[idx_pad] = '<pad>'\n",
    "idx2words_ft[idx_unk] = '<unk>'\n",
    "ordered_words_ft.append('<pad>')\n",
    "ordered_words_ft.append('<unk>')\n",
    "loaded_embeddings_ft[idx_pad] = np.zeros((300,))\n",
    "loaded_embeddings_ft[idx_unk] = random.normal(loc=0.0, scale=1.0, size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = []\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                index_list.append(words_ft[token])\n",
    "            except KeyError:\n",
    "                index_list.append(idx_unk)\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_premise_indices = token2index_dataset(train_premise)\n",
    "train_hypo_indices = token2index_dataset(train_hypo)\n",
    "val_premise_indices = token2index_dataset(val_premise)\n",
    "val_hypo_indices = token2index_dataset(val_hypo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference - Modified from Lab4 in DS 1011 Fall 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class load_dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list_premise,data_list_hypo, target_list):\n",
    "\n",
    "        self.data_list_premise = data_list_premise\n",
    "        self.data_list_hypo = data_list_hypo\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list_premise) == len(self.target_list))\n",
    "        assert (len(self.data_list_hypo) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list_premise)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \n",
    "        token_idx_premise = self.data_list_premise[key]\n",
    "        token_idx_hypo = self.data_list_hypo[key]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx_premise, token_idx_hypo, len(token_idx_premise), len(token_idx_hypo),label]\n",
    "\n",
    "def collate_func(batch):\n",
    "\n",
    "    data_list_premise = []\n",
    "    data_list_hypo = []\n",
    "    label_list = []\n",
    "    length_list_premise = []\n",
    "    length_list_hypo = []\n",
    "\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[4])\n",
    "        length_list_premise.append(datum[2])\n",
    "        length_list_hypo.append(datum[3])\n",
    "    \n",
    "    length_list_premise_sorted = sorted(length_list_premise)\n",
    "    premise_ceiling = length_list_premise_sorted[int(round(len(batch)*0.99))-1]\n",
    "    length_list_hypo_sorted = sorted(length_list_hypo)\n",
    "    hypo_ceiling = length_list_hypo_sorted[int(round(len(batch)*0.99))-1]\n",
    "    \n",
    "    max_premise = 0\n",
    "    max_hypo = 0\n",
    "    for datum in batch:\n",
    "        if datum[2] > max_premise:\n",
    "            max_premise = datum[2]\n",
    "        \n",
    "        if datum[3] > max_hypo:\n",
    "            max_hypo = datum[3]\n",
    "\n",
    "    for datum in batch:\n",
    "\n",
    "        padded_vec_premise = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,max_premise - datum[2])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        padded_vec_hypo = np.pad(np.array(datum[1]), \n",
    "                                pad_width=((0,max_hypo - datum[3])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list_premise.append(padded_vec_premise)\n",
    "        data_list_hypo.append(padded_vec_hypo)\n",
    "        \n",
    "    return [torch.from_numpy(np.array(data_list_premise)), \n",
    "            torch.from_numpy(np.array(data_list_hypo)),\n",
    "            torch.LongTensor(length_list_premise), \n",
    "            torch.LongTensor(length_list_hypo),\n",
    "            torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = load_dataset(train_premise_indices,train_hypo_indices, train_label)\n",
    "val_loader = load_dataset(val_premise_indices, val_hypo_indices, val_label)\n",
    "\n",
    "train_dataset = load_dataset(train_premise_indices,train_hypo_indices, train_label)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=(train_dataset), \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = load_dataset(val_premise_indices,val_hypo_indices, val_label)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=collate_func,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference - Modified from Lab4 in Dpremise011 Fall 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, weight, emb_size, hidden_size, num_layers, num_classes):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # emb_size: Embedding Size\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # num_classes: number of output classes\n",
    "        # vocab_size: vocabulary size\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "                \n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.Tensor(weight), freeze = True).float()\n",
    "        self.rnn = nn.GRU(emb_size, hidden_size,num_layers,batch_first = True, bidirectional = True)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2*hidden_size, hidden_size), \n",
    "            nn.ReLU(inplace=True), \n",
    "            nn.Linear(hidden_size, num_classes))\n",
    "\n",
    "        self.fc1 = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.relu = nn.LeakyReLU(inplace=True)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        hidden = torch.randn(2*self.num_layers, batch_size, self.hidden_size)\n",
    "\n",
    "        return hidden\n",
    "    \n",
    "    \n",
    "    def forward(self, premise, hypo, lengths_premise, lengths_hypo):\n",
    "        # reset hidden state\n",
    "\n",
    "        batch_size_premise, seq_len_premise = premise.size()\n",
    "        batch_size_hypo, seq_len_hypo = hypo.size()\n",
    "\n",
    "        self.hidden_premise = self.init_hidden(batch_size_premise).to(device)\n",
    "        self.hidden_hypo = self.init_hidden(batch_size_hypo).to(device)\n",
    "        \n",
    "        # Compute sorted sequence lengths\n",
    "        _, idx_sort_premise = torch.sort(lengths_premise, dim=0, descending=True)\n",
    "        _, idx_unsort_premise = torch.sort(idx_sort_premise, dim=0)\n",
    "        _, idx_sort_hypo = torch.sort(lengths_hypo, dim=0, descending=True)\n",
    "        _, idx_unsort_hypo = torch.sort(idx_sort_hypo, dim=0)\n",
    "        \n",
    "        lengths_premise = list(lengths_premise[idx_sort_premise])\n",
    "        lengths_hypo = list(lengths_hypo[idx_sort_hypo])\n",
    "\n",
    "        rnn_input_premise = premise.index_select(0, idx_sort_premise)\n",
    "        rnn_input_hypo = hypo.index_select(0, idx_sort_hypo)\n",
    "        \n",
    "        # get embedding of characters\n",
    "        embed_premise = self.embedding(rnn_input_premise)\n",
    "        embed_hypo = self.embedding(rnn_input_hypo)\n",
    "        \n",
    "        embed_premise = torch.nn.utils.rnn.pack_padded_sequence(embed_premise, lengths_premise, batch_first=True)\n",
    "        embed_hypo = torch.nn.utils.rnn.pack_padded_sequence(embed_hypo, lengths_hypo, batch_first=True)\n",
    "        \n",
    "        # fprop though RNN\n",
    "        rnn_out_premise, self.hidden_premise = self.rnn(embed_premise, self.hidden_premise)\n",
    "        rnn_out_hypo, self.hidden_hypo = self.rnn(embed_hypo, self.hidden_hypo)\n",
    "        \n",
    "        hn_premise = torch.sum(self.hidden_premise, dim=0)\n",
    "        hn_hypo = torch.sum(self.hidden_hypo, dim=0)\n",
    "                \n",
    "        output_premise = torch.index_select(hn_premise,0, idx_unsort_premise)\n",
    "        output_hypo = torch.index_select(hn_hypo,0, idx_unsort_hypo)\n",
    "        \n",
    "        rep = output_premise*output_hypo\n",
    "        res = self.fc(rep)\n",
    "        \n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference - Modified from Lab4 in Dpremise011 Fall 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, weight,emb_size, hidden_size, kernel_size, num_classes):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.kernel_size, self.hidden_size = kernel_size, hidden_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.Tensor(weight), freeze = True).float()\n",
    "    \n",
    "        self.conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size=self.kernel_size, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=self.kernel_size, padding=1)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2*hidden_size, hidden_size), \n",
    "            nn.ReLU(inplace=True), \n",
    "            nn.Linear(hidden_size, num_classes))\n",
    "\n",
    "    def forward(self, premise, hypo, lengths_premise, lengths_hypo):\n",
    "        batch_size_premise, seq_len_premise = premise.size()\n",
    "        batch_size_hypo, seq_len_hypo = hypo.size()\n",
    "        \n",
    "        embed_premise = self.embedding(premise).to(device)\n",
    "        embed_hypo = self.embedding(hypo).to(device)\n",
    "       \n",
    "        hidden_premise = self.conv1(embed_premise.transpose(1,2)).transpose(1,2)\n",
    "        hidden_hypo = self.conv1(embed_hypo.transpose(1,2)).transpose(1,2)\n",
    "       \n",
    "        hidden_premise = F.relu(hidden_premise.contiguous().view(-1, hidden_premise.size(-1))).view(batch_size_premise, hidden_premise.size(1), hidden_premise.size(-1))\n",
    "        hidden_hypo = F.relu(hidden_hypo.contiguous().view(-1, hidden_hypo.size(-1))).view(batch_size_hypo, hidden_hypo.size(1), hidden_hypo.size(-1))\n",
    "       \n",
    "        hidden_premise = self.conv2(hidden_premise.transpose(1,2)).transpose(1,2)\n",
    "        hidden_hypo = self.conv2(hidden_hypo.transpose(1,2)).transpose(1,2)\n",
    "        \n",
    "        hidden_premise = F.relu(hidden_premise.contiguous().view(-1, hidden_premise.size(-1))).view(batch_size_premise, hidden_premise.size(1), hidden_premise.size(-1))\n",
    "        hidden_hypo = F.relu(hidden_hypo.contiguous().view(-1, hidden_hypo.size(-1))).view(batch_size_hypo, hidden_hypo.size(1), hidden_hypo.size(-1))\n",
    "       \n",
    "        hidden_premise,hidden_premise_idx = torch.max(hidden_premise, dim=1)\n",
    "        \n",
    "        hidden_hypo,hidden_hypo_idx = torch.max(hidden_hypo, dim=1)\n",
    "        \n",
    "        rep = torch.cat([hidden_premise, hidden_hypo], 1)\n",
    "        output = self.fc(rep)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data_premise, data_hypo, lengths_premise,lengths_hypo, labels in loader:\n",
    "        data_batch_premise, data_batch_hypo, lengths_batch_premise,lengths_batch_hypo, label_batch = data_premise.to(device), data_hypo.to(device), lengths_premise.to(device),lengths_hypo.to(device), labels.to(device)\n",
    "        outputs = F.softmax(model(data_batch_premise, data_batch_hypo, lengths_batch_premise,lengths_batch_hypo), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(label_batch.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference - Modified from Lab4 in Dpremise011 Fall 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(hs,model_name):\n",
    "    best_acc = 0\n",
    "    model = RNN(weight = loaded_embeddings_ft,\n",
    "                emb_size= 300, \n",
    "                hidden_size=hs, \n",
    "                num_layers=1, \n",
    "                num_classes =3).to(device)\n",
    "\n",
    "    learning_rate = 5e-4\n",
    "    num_epochs = 7\n",
    "\n",
    "    # Criterion and Optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the model for premise\n",
    "    total_step = len(train_loader)\n",
    "\n",
    "    val_acc_list_rnn0 = []\n",
    "    train_loss_rnn0 = []\n",
    "    train_acc_list_rnn0 = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (data_premise, data_hypo,lengths_premise, lengths_hypo, labels) in enumerate(train_loader):\n",
    "            data_premise = data_premise.to(device)\n",
    "            data_hypo = data_hypo.to(device)\n",
    "            lengths_premise = lengths_premise.to(device)\n",
    "            lengths_hypo = lengths_hypo.to(device)\n",
    "            labels = labels.to(device)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(data_premise, data_hypo, lengths_premise,lengths_hypo)\n",
    "            loss = criterion(outputs, labels)\n",
    "            train_loss_rnn0.append(loss)\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # validate every 100 iterations\n",
    "            if i > 0 and i % 500 == 0:\n",
    "                # validate\n",
    "                val_acc = test_model(val_loader, model)\n",
    "                train_acc_list_rnn0.append(test_model(train_loader, model))\n",
    "                val_acc_list_rnn0.append(val_acc)\n",
    "                if val_acc > best_acc:\n",
    "                    best_acc = val_acc\n",
    "                    torch.save(model.state_dict(), model_name+'.pth')\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Train Loss: {}, Validation Acc: {}'.format(\n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), loss, val_acc))\n",
    "    return train_acc_list_rnn0,val_acc_list_rnn0,train_loss_rnn0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [501/3125], Validation Acc: 56.2\n",
      "Epoch: [1/10], Step: [1001/3125], Validation Acc: 58.5\n",
      "Epoch: [1/10], Step: [1501/3125], Validation Acc: 57.9\n",
      "Epoch: [1/10], Step: [2001/3125], Validation Acc: 60.0\n",
      "Epoch: [1/10], Step: [2501/3125], Validation Acc: 61.6\n",
      "Epoch: [1/10], Step: [3001/3125], Validation Acc: 62.3\n",
      "Epoch: [2/10], Step: [501/3125], Validation Acc: 64.5\n",
      "Epoch: [2/10], Step: [1001/3125], Validation Acc: 64.0\n",
      "Epoch: [2/10], Step: [1501/3125], Validation Acc: 64.8\n",
      "Epoch: [2/10], Step: [2001/3125], Validation Acc: 64.9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-1dc83245fd91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_acc_list_rnn1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_acc_list_rnn1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loss_rnn1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rnn_hs_300'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'val_acc_list_rnn1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m      \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mval_acc_list_rnn1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_acc_list_rnn1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m      \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc_list_rnn1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-31dbfd1a0bc9>\u001b[0m in \u001b[0;36mtrain_rnn\u001b[0;34m(hs, model_name, element_mul)\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0;31m# validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0mtrain_acc_list_rnn0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0mval_acc_list_rnn0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_acc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-b79e50dacc88>\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(loader, model)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata_s1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_s2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths_s1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlengths_s2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mdata_batch_s1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_batch_s2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths_batch_s1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlengths_batch_s2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_s1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_s2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths_s1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlengths_s2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batch_s1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_batch_s2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths_batch_s1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlengths_batch_s2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/python36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-1736f32fe078>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, s1, s2, lengths_s1, lengths_s2)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# fprop though RNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mrnn_out_s1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_s1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_s1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_s1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mrnn_out_s2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_s2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_s2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_s2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/python36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/python36/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         )\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/python36/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/python36/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, weight, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mnexth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_first\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvariable_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/python36/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight, batch_sizes)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_directions\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 \u001b[0mhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m                 \u001b[0mnext_hidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mall_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/python36/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight, batch_sizes)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mflat_hidden\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/python36/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mGRUCell\u001b[0;34m(input, hidden, w_ih, w_hh, b_ih, b_hh)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mgi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_ih\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_ih\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mgh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_hh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_hh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mi_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mh_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/python36/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_acc_list_rnn1,val_acc_list_rnn1,train_loss_rnn1 = train_rnn(300,'rnn_hs_300')\n",
    "with open('val_acc_list_rnn1', 'wb') as f:\n",
    "     pickle.dump( val_acc_list_rnn1, f)\n",
    "with open('train_acc_list_rnn1', 'wb') as f:\n",
    "     pickle.dump(train_acc_list_rnn1, f)\n",
    "with open('train_loss_rnn1', 'wb') as f:\n",
    "     pickle.dump(train_loss_rnn1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [501/3125], Validation Acc: 56.8\n",
      "Epoch: [1/10], Step: [1001/3125], Validation Acc: 58.1\n",
      "Epoch: [1/10], Step: [1501/3125], Validation Acc: 61.1\n",
      "Epoch: [1/10], Step: [2001/3125], Validation Acc: 59.2\n",
      "Epoch: [1/10], Step: [2501/3125], Validation Acc: 60.4\n",
      "Epoch: [1/10], Step: [3001/3125], Validation Acc: 60.2\n",
      "Epoch: [2/10], Step: [501/3125], Validation Acc: 61.7\n",
      "Epoch: [2/10], Step: [1001/3125], Validation Acc: 62.9\n",
      "Epoch: [2/10], Step: [1501/3125], Validation Acc: 62.8\n",
      "Epoch: [2/10], Step: [2001/3125], Validation Acc: 64.4\n",
      "Epoch: [2/10], Step: [2501/3125], Validation Acc: 65.0\n",
      "Epoch: [2/10], Step: [3001/3125], Validation Acc: 64.8\n",
      "Epoch: [3/10], Step: [501/3125], Validation Acc: 66.8\n",
      "Epoch: [3/10], Step: [1001/3125], Validation Acc: 66.5\n",
      "Epoch: [3/10], Step: [1501/3125], Validation Acc: 64.3\n",
      "Epoch: [3/10], Step: [2001/3125], Validation Acc: 66.5\n",
      "Epoch: [3/10], Step: [2501/3125], Validation Acc: 67.7\n",
      "Epoch: [3/10], Step: [3001/3125], Validation Acc: 65.7\n",
      "Epoch: [4/10], Step: [501/3125], Validation Acc: 66.6\n",
      "Epoch: [4/10], Step: [1001/3125], Validation Acc: 66.8\n",
      "Epoch: [4/10], Step: [1501/3125], Validation Acc: 67.3\n",
      "Epoch: [4/10], Step: [2001/3125], Validation Acc: 67.6\n",
      "Epoch: [4/10], Step: [2501/3125], Validation Acc: 67.8\n",
      "Epoch: [4/10], Step: [3001/3125], Validation Acc: 69.0\n",
      "Epoch: [5/10], Step: [501/3125], Validation Acc: 69.4\n",
      "Epoch: [5/10], Step: [1001/3125], Validation Acc: 67.1\n",
      "Epoch: [5/10], Step: [1501/3125], Validation Acc: 68.8\n",
      "Epoch: [5/10], Step: [2001/3125], Validation Acc: 69.5\n",
      "Epoch: [5/10], Step: [2501/3125], Validation Acc: 69.6\n",
      "Epoch: [5/10], Step: [3001/3125], Validation Acc: 67.3\n",
      "Epoch: [6/10], Step: [501/3125], Validation Acc: 70.0\n",
      "Epoch: [6/10], Step: [1001/3125], Validation Acc: 69.3\n",
      "Epoch: [6/10], Step: [1501/3125], Validation Acc: 69.0\n",
      "Epoch: [6/10], Step: [2001/3125], Validation Acc: 68.2\n",
      "Epoch: [6/10], Step: [2501/3125], Validation Acc: 70.2\n",
      "Epoch: [6/10], Step: [3001/3125], Validation Acc: 69.7\n",
      "Epoch: [7/10], Step: [501/3125], Validation Acc: 70.2\n",
      "Epoch: [7/10], Step: [1001/3125], Validation Acc: 70.6\n",
      "Epoch: [7/10], Step: [1501/3125], Validation Acc: 70.9\n",
      "Epoch: [7/10], Step: [2001/3125], Validation Acc: 70.6\n",
      "Epoch: [7/10], Step: [2501/3125], Validation Acc: 70.4\n",
      "Epoch: [7/10], Step: [3001/3125], Validation Acc: 71.1\n",
      "Epoch: [8/10], Step: [501/3125], Validation Acc: 71.0\n",
      "Epoch: [8/10], Step: [1001/3125], Validation Acc: 69.5\n",
      "Epoch: [8/10], Step: [1501/3125], Validation Acc: 70.4\n",
      "Epoch: [8/10], Step: [2001/3125], Validation Acc: 72.0\n",
      "Epoch: [8/10], Step: [2501/3125], Validation Acc: 71.7\n",
      "Epoch: [8/10], Step: [3001/3125], Validation Acc: 72.1\n",
      "Epoch: [9/10], Step: [501/3125], Validation Acc: 71.3\n",
      "Epoch: [9/10], Step: [1001/3125], Validation Acc: 71.6\n",
      "Epoch: [9/10], Step: [1501/3125], Validation Acc: 70.6\n",
      "Epoch: [9/10], Step: [2001/3125], Validation Acc: 72.3\n",
      "Epoch: [9/10], Step: [2501/3125], Validation Acc: 72.5\n",
      "Epoch: [9/10], Step: [3001/3125], Validation Acc: 71.8\n",
      "Epoch: [10/10], Step: [501/3125], Validation Acc: 71.0\n",
      "Epoch: [10/10], Step: [1001/3125], Validation Acc: 72.4\n",
      "Epoch: [10/10], Step: [1501/3125], Validation Acc: 72.5\n",
      "Epoch: [10/10], Step: [2001/3125], Validation Acc: 71.8\n",
      "Epoch: [10/10], Step: [2501/3125], Validation Acc: 72.1\n",
      "Epoch: [10/10], Step: [3001/3125], Validation Acc: 71.9\n"
     ]
    }
   ],
   "source": [
    "train_acc_list_rnn3,val_acc_list_rnn3,train_loss_rnn3 = train_rnn(100,'rnn_hs_100')\n",
    "with open('val_acc_list_rnn2', 'wb') as f:\n",
    "     pickle.dump( val_acc_list_rnn3, f)\n",
    "with open('train_acc_list_rnn2', 'wb') as f:\n",
    "     pickle.dump(train_acc_list_rnn3, f)\n",
    "with open('train_loss_rnn2', 'wb') as f:\n",
    "     pickle.dump(train_loss_rnn3, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Run CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference - Modified from Lab4 in Dpremise011 Fall 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_cnn(hs,ks,model_name):    \n",
    "    best_acc = 0\n",
    "\n",
    "    model = CNN(weight = loaded_embeddings_ft,\n",
    "                emb_size = 300, \n",
    "                hidden_size=hs, \n",
    "                kernel_size=ks, \n",
    "                num_classes =3).to(device)\n",
    "\n",
    "    learning_rate = 5e-4\n",
    "    num_epochs = 7\n",
    "\n",
    "    # Criterion and Optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the model for premise\n",
    "    total_step = len(train_loader)\n",
    "\n",
    "    val_acc_list_cnn0 = []\n",
    "    train_loss_cnn0 = []\n",
    "    train_acc_list_cnn0 = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (data_premise, data_hypo,lengths_premise, lengths_hypo, labels) in enumerate(train_loader):\n",
    "            data_premise = data_premise.to(device)\n",
    "            data_hypo = data_hypo.to(device)\n",
    "            lengths_premise = lengths_premise.to(device)\n",
    "            lengths_hypo = lengths_hypo.to(device)\n",
    "            labels = labels.to(device)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(data_premise, data_hypo, lengths_premise,lengths_hypo)\n",
    "            loss = criterion(outputs, labels)\n",
    "            train_loss_cnn0.append(loss)\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # validate every 100 iterations\n",
    "            if i > 0 and i % 500 == 0:\n",
    "                # validate\n",
    "                val_acc = test_model(val_loader, model)\n",
    "                train_acc = test_model(train_loader, model)\n",
    "                train_acc_list_cnn0.append(train_acc)\n",
    "                val_acc_list_cnn0.append(val_acc)\n",
    "                if val_acc > best_acc:\n",
    "                    best_acc = val_acc\n",
    "                    torch.save(model.state_dict(), model_name+'.pth''.pth')\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Train Loss: {}, Validation Acc: {}'.format(\n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), loss, val_acc))\n",
    "    return train_acc_list_cnn0,val_acc_list_cnn0,train_loss_cnn0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [501/3125], Train Loss: 0.9574393033981323, Validation Acc: 42.1\n",
      "Epoch: [1/10], Step: [1001/3125], Train Loss: 0.9434483051300049, Validation Acc: 58.0\n",
      "Epoch: [1/10], Step: [1501/3125], Train Loss: 0.8900450468063354, Validation Acc: 58.2\n",
      "Epoch: [1/10], Step: [2001/3125], Train Loss: 0.8556941747665405, Validation Acc: 59.2\n",
      "Epoch: [1/10], Step: [2501/3125], Train Loss: 0.7658823728561401, Validation Acc: 60.7\n",
      "Epoch: [1/10], Step: [3001/3125], Train Loss: 0.7631287574768066, Validation Acc: 60.9\n",
      "Epoch: [2/10], Step: [501/3125], Train Loss: 0.7859179973602295, Validation Acc: 61.4\n",
      "Epoch: [2/10], Step: [1001/3125], Train Loss: 0.9447571635246277, Validation Acc: 61.2\n",
      "Epoch: [2/10], Step: [1501/3125], Train Loss: 0.9533923268318176, Validation Acc: 61.5\n",
      "Epoch: [2/10], Step: [2001/3125], Train Loss: 0.8006707429885864, Validation Acc: 64.0\n",
      "Epoch: [2/10], Step: [2501/3125], Train Loss: 0.6546100974082947, Validation Acc: 63.1\n",
      "Epoch: [2/10], Step: [3001/3125], Train Loss: 0.8021379113197327, Validation Acc: 63.0\n",
      "Epoch: [3/10], Step: [501/3125], Train Loss: 0.8142142295837402, Validation Acc: 62.7\n",
      "Epoch: [3/10], Step: [1001/3125], Train Loss: 0.6387206315994263, Validation Acc: 63.2\n",
      "Epoch: [3/10], Step: [1501/3125], Train Loss: 0.86528080701828, Validation Acc: 65.5\n",
      "Epoch: [3/10], Step: [2001/3125], Train Loss: 0.6741074323654175, Validation Acc: 65.2\n",
      "Epoch: [3/10], Step: [2501/3125], Train Loss: 0.7050051093101501, Validation Acc: 65.4\n",
      "Epoch: [3/10], Step: [3001/3125], Train Loss: 0.5878629684448242, Validation Acc: 65.5\n",
      "Epoch: [4/10], Step: [501/3125], Train Loss: 0.7882422208786011, Validation Acc: 66.7\n",
      "Epoch: [4/10], Step: [1001/3125], Train Loss: 0.8658557534217834, Validation Acc: 66.5\n",
      "Epoch: [4/10], Step: [1501/3125], Train Loss: 0.8333885669708252, Validation Acc: 64.6\n",
      "Epoch: [4/10], Step: [2001/3125], Train Loss: 0.6394285559654236, Validation Acc: 66.7\n",
      "Epoch: [4/10], Step: [2501/3125], Train Loss: 0.780461311340332, Validation Acc: 67.4\n",
      "Epoch: [4/10], Step: [3001/3125], Train Loss: 1.099218487739563, Validation Acc: 67.6\n",
      "Epoch: [5/10], Step: [501/3125], Train Loss: 0.8551314473152161, Validation Acc: 68.1\n",
      "Epoch: [5/10], Step: [1001/3125], Train Loss: 0.5369775891304016, Validation Acc: 67.6\n",
      "Epoch: [5/10], Step: [1501/3125], Train Loss: 0.8341291546821594, Validation Acc: 67.7\n",
      "Epoch: [5/10], Step: [2001/3125], Train Loss: 0.5627148747444153, Validation Acc: 68.7\n",
      "Epoch: [5/10], Step: [2501/3125], Train Loss: 0.4324294924736023, Validation Acc: 65.5\n",
      "Epoch: [5/10], Step: [3001/3125], Train Loss: 0.837483823299408, Validation Acc: 66.3\n",
      "Epoch: [6/10], Step: [501/3125], Train Loss: 0.4438420236110687, Validation Acc: 67.3\n",
      "Epoch: [6/10], Step: [1001/3125], Train Loss: 0.6828787922859192, Validation Acc: 68.0\n",
      "Epoch: [6/10], Step: [1501/3125], Train Loss: 0.5251795053482056, Validation Acc: 67.5\n",
      "Epoch: [6/10], Step: [2001/3125], Train Loss: 0.6818469762802124, Validation Acc: 67.9\n",
      "Epoch: [6/10], Step: [2501/3125], Train Loss: 0.6400378346443176, Validation Acc: 67.8\n",
      "Epoch: [6/10], Step: [3001/3125], Train Loss: 0.45880451798439026, Validation Acc: 67.9\n",
      "Epoch: [7/10], Step: [501/3125], Train Loss: 0.5577507615089417, Validation Acc: 68.1\n",
      "Epoch: [7/10], Step: [1001/3125], Train Loss: 5.625737190246582, Validation Acc: 68.4\n",
      "Epoch: [7/10], Step: [1501/3125], Train Loss: 0.5585699081420898, Validation Acc: 68.0\n",
      "Epoch: [7/10], Step: [2001/3125], Train Loss: 0.5608311891555786, Validation Acc: 67.0\n",
      "Epoch: [7/10], Step: [2501/3125], Train Loss: 0.3892887234687805, Validation Acc: 68.6\n",
      "Epoch: [7/10], Step: [3001/3125], Train Loss: 0.5644186735153198, Validation Acc: 69.3\n",
      "Epoch: [8/10], Step: [501/3125], Train Loss: 0.5288856625556946, Validation Acc: 69.1\n",
      "Epoch: [8/10], Step: [1001/3125], Train Loss: 0.7870170474052429, Validation Acc: 68.4\n",
      "Epoch: [8/10], Step: [1501/3125], Train Loss: 0.31858158111572266, Validation Acc: 67.8\n",
      "Epoch: [8/10], Step: [2001/3125], Train Loss: 0.4259966015815735, Validation Acc: 67.4\n",
      "Epoch: [8/10], Step: [2501/3125], Train Loss: 0.4236169159412384, Validation Acc: 69.3\n",
      "Epoch: [8/10], Step: [3001/3125], Train Loss: 0.5861362814903259, Validation Acc: 66.7\n",
      "Epoch: [9/10], Step: [501/3125], Train Loss: 0.25207608938217163, Validation Acc: 67.1\n",
      "Epoch: [9/10], Step: [1001/3125], Train Loss: 0.45061084628105164, Validation Acc: 68.6\n",
      "Epoch: [9/10], Step: [1501/3125], Train Loss: 0.22091004252433777, Validation Acc: 68.6\n",
      "Epoch: [9/10], Step: [2001/3125], Train Loss: 0.4483623802661896, Validation Acc: 68.8\n",
      "Epoch: [9/10], Step: [2501/3125], Train Loss: 0.4184027910232544, Validation Acc: 68.1\n",
      "Epoch: [9/10], Step: [3001/3125], Train Loss: 0.625960648059845, Validation Acc: 68.9\n",
      "Epoch: [10/10], Step: [501/3125], Train Loss: 0.4221455752849579, Validation Acc: 66.8\n",
      "Epoch: [10/10], Step: [1001/3125], Train Loss: 0.34138545393943787, Validation Acc: 68.0\n",
      "Epoch: [10/10], Step: [1501/3125], Train Loss: 0.3107822835445404, Validation Acc: 68.0\n",
      "Epoch: [10/10], Step: [2001/3125], Train Loss: 0.37277844548225403, Validation Acc: 67.6\n",
      "Epoch: [10/10], Step: [2501/3125], Train Loss: 0.6414076685905457, Validation Acc: 69.7\n",
      "Epoch: [10/10], Step: [3001/3125], Train Loss: 0.30171293020248413, Validation Acc: 67.3\n"
     ]
    }
   ],
   "source": [
    "train_acc_list_cnn2,val_acc_list_cnn2,train_loss_cnn2 = train_cnn(400,3,'cnn_hs_400_ks_3')\n",
    "with open('val_acc_list_cnn1', 'wb') as f:\n",
    "     pickle.dump( val_acc_list_cnn2, f)\n",
    "with open('train_acc_list_cnn1', 'wb') as f:\n",
    "     pickle.dump(train_acc_list_cnn2, f)\n",
    "with open('train_loss_cnn1', 'wb') as f:\n",
    "     pickle.dump(train_loss_cnn2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [501/3125], Train Loss: 1.0572879314422607, Validation Acc: 42.2\n",
      "Epoch: [1/10], Step: [1001/3125], Train Loss: 0.9764376878738403, Validation Acc: 55.4\n",
      "Epoch: [1/10], Step: [1501/3125], Train Loss: 0.8318452835083008, Validation Acc: 56.9\n",
      "Epoch: [1/10], Step: [2001/3125], Train Loss: 0.7551078200340271, Validation Acc: 58.2\n",
      "Epoch: [1/10], Step: [2501/3125], Train Loss: 0.8722537755966187, Validation Acc: 58.9\n",
      "Epoch: [1/10], Step: [3001/3125], Train Loss: 1.0347135066986084, Validation Acc: 60.2\n",
      "Epoch: [2/10], Step: [501/3125], Train Loss: 0.834957480430603, Validation Acc: 60.4\n",
      "Epoch: [2/10], Step: [1001/3125], Train Loss: 0.9010812044143677, Validation Acc: 60.0\n",
      "Epoch: [2/10], Step: [1501/3125], Train Loss: 0.8308014869689941, Validation Acc: 61.1\n",
      "Epoch: [2/10], Step: [2001/3125], Train Loss: 0.7764003276824951, Validation Acc: 59.9\n",
      "Epoch: [2/10], Step: [2501/3125], Train Loss: 0.8452682495117188, Validation Acc: 60.5\n",
      "Epoch: [2/10], Step: [3001/3125], Train Loss: 0.9266514182090759, Validation Acc: 60.9\n",
      "Epoch: [3/10], Step: [501/3125], Train Loss: 0.6993072628974915, Validation Acc: 60.1\n",
      "Epoch: [3/10], Step: [1001/3125], Train Loss: 0.7272104024887085, Validation Acc: 61.7\n",
      "Epoch: [3/10], Step: [1501/3125], Train Loss: 0.8487998247146606, Validation Acc: 62.7\n",
      "Epoch: [3/10], Step: [2001/3125], Train Loss: 0.6822755932807922, Validation Acc: 60.4\n",
      "Epoch: [3/10], Step: [2501/3125], Train Loss: 0.9097238779067993, Validation Acc: 60.0\n",
      "Epoch: [3/10], Step: [3001/3125], Train Loss: 0.9128665328025818, Validation Acc: 63.7\n",
      "Epoch: [4/10], Step: [501/3125], Train Loss: 1.0707212686538696, Validation Acc: 63.3\n",
      "Epoch: [4/10], Step: [1001/3125], Train Loss: 0.8042099475860596, Validation Acc: 63.9\n",
      "Epoch: [4/10], Step: [1501/3125], Train Loss: 0.5329689383506775, Validation Acc: 63.6\n",
      "Epoch: [4/10], Step: [2001/3125], Train Loss: 1.0676312446594238, Validation Acc: 64.5\n",
      "Epoch: [4/10], Step: [2501/3125], Train Loss: 0.8641377091407776, Validation Acc: 63.8\n",
      "Epoch: [4/10], Step: [3001/3125], Train Loss: 0.802510678768158, Validation Acc: 64.1\n",
      "Epoch: [5/10], Step: [501/3125], Train Loss: 0.8459120988845825, Validation Acc: 61.2\n",
      "Epoch: [5/10], Step: [1001/3125], Train Loss: 0.9564571976661682, Validation Acc: 63.8\n",
      "Epoch: [5/10], Step: [1501/3125], Train Loss: 0.642386794090271, Validation Acc: 63.4\n",
      "Epoch: [5/10], Step: [2001/3125], Train Loss: 0.8132192492485046, Validation Acc: 64.6\n",
      "Epoch: [5/10], Step: [2501/3125], Train Loss: 0.814164400100708, Validation Acc: 64.4\n",
      "Epoch: [5/10], Step: [3001/3125], Train Loss: 0.8803533911705017, Validation Acc: 64.4\n",
      "Epoch: [6/10], Step: [501/3125], Train Loss: 0.8405492901802063, Validation Acc: 64.5\n",
      "Epoch: [6/10], Step: [1001/3125], Train Loss: 0.5566574931144714, Validation Acc: 65.5\n",
      "Epoch: [6/10], Step: [1501/3125], Train Loss: 0.7829983830451965, Validation Acc: 64.2\n",
      "Epoch: [6/10], Step: [2001/3125], Train Loss: 0.6283023953437805, Validation Acc: 65.8\n",
      "Epoch: [6/10], Step: [2501/3125], Train Loss: 0.7726981043815613, Validation Acc: 65.2\n",
      "Epoch: [6/10], Step: [3001/3125], Train Loss: 1.032499074935913, Validation Acc: 65.5\n",
      "Epoch: [7/10], Step: [501/3125], Train Loss: 0.73988938331604, Validation Acc: 64.4\n",
      "Epoch: [7/10], Step: [1001/3125], Train Loss: 0.5533459186553955, Validation Acc: 65.6\n",
      "Epoch: [7/10], Step: [1501/3125], Train Loss: 0.6880446672439575, Validation Acc: 65.5\n",
      "Epoch: [7/10], Step: [2001/3125], Train Loss: 0.4379615783691406, Validation Acc: 65.5\n",
      "Epoch: [7/10], Step: [2501/3125], Train Loss: 0.5469133853912354, Validation Acc: 64.7\n",
      "Epoch: [7/10], Step: [3001/3125], Train Loss: 0.5836999416351318, Validation Acc: 64.3\n",
      "Epoch: [8/10], Step: [501/3125], Train Loss: 0.7940847873687744, Validation Acc: 65.8\n",
      "Epoch: [8/10], Step: [1001/3125], Train Loss: 0.431720495223999, Validation Acc: 64.5\n",
      "Epoch: [8/10], Step: [1501/3125], Train Loss: 0.5657039880752563, Validation Acc: 65.7\n",
      "Epoch: [8/10], Step: [2001/3125], Train Loss: 0.3828044533729553, Validation Acc: 64.7\n",
      "Epoch: [8/10], Step: [2501/3125], Train Loss: 42.02397155761719, Validation Acc: 41.0\n",
      "Epoch: [8/10], Step: [3001/3125], Train Loss: 0.8756099343299866, Validation Acc: 64.5\n",
      "Epoch: [9/10], Step: [501/3125], Train Loss: 0.7432066202163696, Validation Acc: 65.9\n",
      "Epoch: [9/10], Step: [1001/3125], Train Loss: 0.4599010646343231, Validation Acc: 64.4\n",
      "Epoch: [9/10], Step: [1501/3125], Train Loss: 0.776170015335083, Validation Acc: 65.0\n",
      "Epoch: [9/10], Step: [2001/3125], Train Loss: 0.8086938858032227, Validation Acc: 67.7\n",
      "Epoch: [9/10], Step: [2501/3125], Train Loss: 0.9406205415725708, Validation Acc: 66.4\n",
      "Epoch: [9/10], Step: [3001/3125], Train Loss: 0.5830138921737671, Validation Acc: 66.3\n",
      "Epoch: [10/10], Step: [501/3125], Train Loss: 0.5733580589294434, Validation Acc: 65.8\n",
      "Epoch: [10/10], Step: [1001/3125], Train Loss: 0.640480101108551, Validation Acc: 65.6\n",
      "Epoch: [10/10], Step: [1501/3125], Train Loss: 0.5304068922996521, Validation Acc: 66.4\n",
      "Epoch: [10/10], Step: [2001/3125], Train Loss: 0.5101627111434937, Validation Acc: 67.1\n",
      "Epoch: [10/10], Step: [2501/3125], Train Loss: 0.7510788440704346, Validation Acc: 66.6\n",
      "Epoch: [10/10], Step: [3001/3125], Train Loss: 0.6351895928382874, Validation Acc: 66.4\n"
     ]
    }
   ],
   "source": [
    "train_acc_list_cnn3,val_acc_list_cnn3,train_loss_cnn3 = train_cnn(100,2,'cnn_hs_100_ks_2')\n",
    "with open('val_acc_list_cnn2', 'wb') as f:\n",
    "     pickle.dump( val_acc_list_cnn3, f)\n",
    "with open('train_acc_list_cnn2', 'wb') as f:\n",
    "     pickle.dump(train_acc_list_cnn3, f)\n",
    "with open('train_loss_cnn2', 'wb') as f:\n",
    "     pickle.dump(train_loss_cnn3, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pickle.load(open( str(\"val_acc_list_cnn1\"), \"rb\" ))\n",
    "data2 = pickle.load(open( str(\"train_acc_list_cnn1\"), \"rb\" ))\n",
    "data3 = pickle.load(open( str(\"train_loss_cnn1\"), \"rb\" ))\n",
    "data4 = pickle.load(open( str(\"val_acc_list_cnn2\"), \"rb\" ))\n",
    "data5 = pickle.load(open( str(\"train_acc_list_cnn2\"), \"rb\" ))\n",
    "data6 = pickle.load(open( str(\"train_loss_cnn2\"), \"rb\" ))\n",
    "\n",
    "data7 = pickle.load(open( str(\"val_acc_list_rnn1\"), \"rb\" ))\n",
    "data8 = pickle.load(open( str(\"train_acc_list_rnn1\"), \"rb\" ))\n",
    "data9 = pickle.load(open( str(\"train_loss_rnn1\"), \"rb\" ))\n",
    "data10 = pickle.load(open( str(\"val_acc_list_rnn2\"), \"rb\" ))\n",
    "data11 = pickle.load(open( str(\"train_acc_list_rnn2\"), \"rb\" ))\n",
    "data12 = pickle.load(open( str(\"train_loss_rnn2\"), \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(42), data1, label = 'val_acc_list_cnn1')\n",
    "plt.plot(range(42), data2, label = 'train_acc_list_cnn1')\n",
    "plt.plot(range(42), data3, label = 'train_loss_cnn1')\n",
    "plt.plot(range(42), data4, label = 'val_acc_list_cnn2')\n",
    "plt.plot(range(42), data5, label = 'train_acc_list_cnn2')\n",
    "plt.plot(range(42), data6, label = 'train_loss_cnn2')\n",
    "plt.xlabel('Step*100')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(42), data1, label = 'val_acc_list_rnn1')\n",
    "plt.plot(range(42), data2, label = 'train_acc_list_rnn1')\n",
    "plt.plot(range(42), data3, label = 'train_loss_rnn1')\n",
    "plt.plot(range(42), data4, label = 'val_acc_list_rnn2')\n",
    "plt.plot(range(42), data5, label = 'train_acc_list_rnn2')\n",
    "plt.plot(range(42), data6, label = 'train_loss_rnn2')\n",
    "plt.xlabel('Step*100')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MultiNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_mnli(filename):\n",
    "    train_premise = []\n",
    "    train_hypo = []\n",
    "    train_genre = []\n",
    "    train_label = []\n",
    "    with open(filename) as tsvfile:\n",
    "      fd = csv.reader(tsvfile, delimiter='\\t')\n",
    "      for row in fd:\n",
    "            \n",
    "        train_premise.append(row[0].split())\n",
    "        train_hypo.append(row[1].split())\n",
    "        \n",
    "        if row[2] == 'neutral': \n",
    "            train_label.append(1)\n",
    "        elif row[2] == 'entailment':\n",
    "            train_label.append(2)\n",
    "        else:\n",
    "            train_label.append(0)\n",
    "        \n",
    "        train_genre.append(row[3])\n",
    "        \n",
    "    train_premise = train_premise[1:]\n",
    "    train_hypo = train_hypo[1:]\n",
    "    \n",
    "    train_label = train_label[1:]\n",
    "    train_genre = train_genre[1:]\n",
    "    \n",
    "    return train_premise,train_hypo, train_label, train_genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_premise,test_hypo,test_label,test_genre = load_mnli('mnli_val.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_list = list(pd.Series(test_genre).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_mnli_by_genre(filename,genre):\n",
    "    \n",
    "    premise = []\n",
    "    hypo = []\n",
    "    label = []\n",
    "    with open(filename) as tsvfile:\n",
    "      fd = csv.reader(tsvfile, delimiter='\\t')\n",
    "      for row in fd:\n",
    "        \n",
    "        if row[3] == genre:\n",
    "            if len(row[0].split())<1000 and len(row[1].split())<1000:\n",
    "                premise.append(row[0].split())\n",
    "                hypo.append(row[1].split())\n",
    "\n",
    "                if row[2] == 'neutral': \n",
    "                    label.append(1)\n",
    "                elif row[2] == 'entailment':\n",
    "                    label.append(2)\n",
    "                else:\n",
    "                    label.append(0)\n",
    "\n",
    "    return premise,hypo,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "RNN_acc = []\n",
    "CNN_acc = []\n",
    "for genre in genre_list:\n",
    "    test_premise,test_hypo,test_label = load_mnli_by_genre('mnli_val.tsv',genre)\n",
    "    test_premise_indices = token2index_dataset(test_premise)\n",
    "    test_hypo_indices = token2index_dataset(test_hypo)\n",
    "    test_dataset = load_dataset(test_premise_indices,test_hypo_indices, test_label)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                               batch_size=32,\n",
    "                                               collate_fn=collate_func,\n",
    "                                               shuffle=True)  \n",
    "\n",
    "    model_rnn = RNN(weight = loaded_embeddings_ft,\n",
    "                    emb_size= 300, \n",
    "                    hidden_size=400, \n",
    "                    num_layers=1, \n",
    "                    num_classes =3).to(device)\n",
    "    model_rnn.load_state_dict(torch.load(\"rnn_hs_300.pth\",map_location = \"cpu\"))\n",
    "    model_rnn.eval()\n",
    "\n",
    "    model_cnn = CNN(weight = loaded_embeddings_ft,\n",
    "                    emb_size= 300, \n",
    "                    hidden_size=400, \n",
    "                    kernel_size=2, \n",
    "                    num_classes =3\n",
    "                    ).to(device)\n",
    "    model_cnn.load_state_dict(torch.load(\"cnn_hs_400_ks_3.pth\", map_location = \"cpu\"))\n",
    "    model_cnn.eval()\n",
    "    \n",
    "    rnn_test = test_model(test_loader, model_rnn)\n",
    "    cnn_test = test_model(test_loader, model_cnn)\n",
    "    RNN_acc.append(rnn_test)\n",
    "    CNN_acc.append(cnn_test)\n",
    "    print('RNN with',genre,': ', round(rnn_test, 2))\n",
    "    print('CNN with',genre,': ', round(cnn_test, 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
